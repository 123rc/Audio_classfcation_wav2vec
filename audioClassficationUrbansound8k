{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":928025,"sourceType":"datasetVersion","datasetId":500970}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport pandas as pd\nimport torchaudio\nimport os\nimport torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-02T14:00:34.540959Z","iopub.execute_input":"2024-07-02T14:00:34.541826Z","iopub.status.idle":"2024-07-02T14:00:39.803645Z","shell.execute_reply.started":"2024-07-02T14:00:34.541790Z","shell.execute_reply":"2024-07-02T14:00:39.802422Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class UrbanSoundDataset(Dataset):\n\n    def __init__(self,\n                 annotations_file,\n                 audio_dir,\n                 transformation,\n                 target_sample_rate,\n                 num_samples,\n                 device):\n        self.annotations = pd.read_csv(annotations_file)\n        self.audio_dir = audio_dir\n        self.device = device\n        self.transformation = transformation.to(self.device)\n        self.target_sample_rate = target_sample_rate\n        self.num_samples = num_samples\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        audio_sample_path = self._get_audio_sample_path(index)\n        label = self._get_audio_sample_label(index)\n        signal, sr = torchaudio.load(audio_sample_path)\n        signal = signal.to(self.device)\n        signal = self._resample_if_necessary(signal, sr)\n        signal = self._mix_down_if_necessary(signal)\n        signal = self._cut_if_necessary(signal)\n        signal = self._right_pad_if_necessary(signal)\n        signal = self.transformation(signal)\n        return signal, label\n\n    def _cut_if_necessary(self, signal):\n        if signal.shape[1] > self.num_samples:\n            signal = signal[:, :self.num_samples]\n        return signal\n\n    def _right_pad_if_necessary(self, signal):\n        length_signal = signal.shape[1]\n        if length_signal < self.num_samples:\n            num_missing_samples = self.num_samples - length_signal\n            last_dim_padding = (0, num_missing_samples)\n            signal = torch.nn.functional.pad(signal, last_dim_padding)\n        return signal\n\n    def _resample_if_necessary(self, signal, sr):\n        if sr != self.target_sample_rate:\n            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n            signal = resampler(signal)\n        return signal\n\n    def _mix_down_if_necessary(self, signal):\n        if signal.shape[0] > 1:\n            signal = torch.mean(signal, dim=0, keepdim=True)\n        return signal\n\n    def _get_audio_sample_path(self, index):\n        fold = f\"fold{self.annotations.iloc[index, 5]}\"\n        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[\n            index, 0])\n        return path\n\n    def _get_audio_sample_label(self, index):\n        return self.annotations.iloc[index, 6]\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T14:00:39.805987Z","iopub.execute_input":"2024-07-02T14:00:39.806903Z","iopub.status.idle":"2024-07-02T14:00:39.821383Z","shell.execute_reply.started":"2024-07-02T14:00:39.806866Z","shell.execute_reply":"2024-07-02T14:00:39.819794Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    ANNOTATIONS_FILE = \"/kaggle/input/urbansound8k/UrbanSound8K.csv\"\n    AUDIO_DIR = \"/kaggle/input/urbansound8k\"\n    SAMPLE_RATE = 22050\n    NUM_SAMPLES = 22050\n\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n    print(f\"Using device {device}\")\n\n    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n        sample_rate=SAMPLE_RATE,\n        n_fft=1024,\n        hop_length=512,\n        n_mels=64\n    )\n\n    usd = UrbanSoundDataset(ANNOTATIONS_FILE,\n                            AUDIO_DIR,\n                            mel_spectrogram,\n                            SAMPLE_RATE,\n                            NUM_SAMPLES,\n                            device)\n    print(f\"There are {len(usd)} samples in the dataset.\")\n    signal, label = usd[0]","metadata":{"execution":{"iopub.status.busy":"2024-07-02T14:01:06.802960Z","iopub.execute_input":"2024-07-02T14:01:06.803422Z","iopub.status.idle":"2024-07-02T14:01:06.844053Z","shell.execute_reply.started":"2024-07-02T14:01:06.803388Z","shell.execute_reply":"2024-07-02T14:01:06.842654Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Using device cpu\nThere are 8732 samples in the dataset.\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch import nn\nfrom torchsummary import summary\n\n\nclass CNNNetwork(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # 4 conv blocks / flatten / linear / softmax\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=1,\n                out_channels=16,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=16,\n                out_channels=32,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=32,\n                out_channels=64,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(\n                in_channels=64,\n                out_channels=128,\n                kernel_size=3,\n                stride=1,\n                padding=2\n            ),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        self.flatten = nn.Flatten()\n        self.linear = nn.Linear(128 * 5 * 4, 10)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, input_data):\n        x = self.conv1(input_data)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.flatten(x)\n        logits = self.linear(x)\n        predictions = self.softmax(logits)\n        return predictions\n\n\nif __name__ == \"__main__\":\n    cnn = CNNNetwork()\n    summary(cnn.cuda(), (1, 64, 44))","metadata":{"execution":{"iopub.status.busy":"2024-07-02T14:27:14.280767Z","iopub.execute_input":"2024-07-02T14:27:14.281474Z","iopub.status.idle":"2024-07-02T14:27:15.039506Z","shell.execute_reply.started":"2024-07-02T14:27:14.281409Z","shell.execute_reply":"2024-07-02T14:27:15.037587Z"},"trusted":true},"execution_count":8,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 71\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     70\u001b[0m     cnn \u001b[38;5;241m=\u001b[39m CNNNetwork()\n\u001b[0;32m---> 71\u001b[0m     summary(\u001b[43mcnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m44\u001b[39m))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:918\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    902\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:918\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    902\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/__init__.py:289\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m     )\n","\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"],"ename":"AssertionError","evalue":"Torch not compiled with CUDA enabled","output_type":"error"}]},{"cell_type":"code","source":"BATCH_SIZE = 128\nEPOCHS = 10\nLEARNING_RATE = 0.001\n\nANNOTATIONS_FILE = \"/home/valerio/datasets/UrbanSound8K/metadata/UrbanSound8K.csv\"\nAUDIO_DIR = \"/home/valerio/datasets/UrbanSound8K/audio\"\nSAMPLE_RATE = 22050\nNUM_SAMPLES = 22050\n\n\ndef create_data_loader(train_data, batch_size):\n    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n    return train_dataloader\n\n\ndef train_single_epoch(model, data_loader, loss_fn, optimiser, device):\n    for input, target in data_loader:\n        input, target = input.to(device), target.to(device)\n\n        # calculate loss\n        prediction = model(input)\n        loss = loss_fn(prediction, target)\n\n        # backpropagate error and update weights\n        optimiser.zero_grad()\n        loss.backward()\n        optimiser.step()\n\n    print(f\"loss: {loss.item()}\")\n\n\ndef train(model, data_loader, loss_fn, optimiser, device, epochs):\n    for i in range(epochs):\n        print(f\"Epoch {i+1}\")\n        train_single_epoch(model, data_loader, loss_fn, optimiser, device)\n        print(\"---------------------------\")\n    print(\"Finished training\")\n\n\nif __name__ == \"__main__\":\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n    print(f\"Using {device}\")\n\n    # instantiating our dataset object and create data loader\n    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n        sample_rate=SAMPLE_RATE,\n        n_fft=1024,\n        hop_length=512,\n        n_mels=64\n    )\n\n    usd = UrbanSoundDataset(ANNOTATIONS_FILE,\n                            AUDIO_DIR,\n                            mel_spectrogram,\n                            SAMPLE_RATE,\n                            NUM_SAMPLES,\n                            device)\n    \n    train_dataloader = create_data_loader(usd, BATCH_SIZE)\n\n    # construct model and assign it to device\n    cnn = CNNNetwork().to(device)\n    print(cnn)\n\n    # initialise loss funtion + optimiser\n    loss_fn = nn.CrossEntropyLoss()\n    optimiser = torch.optim.Adam(cnn.parameters(),\n                                 lr=LEARNING_RATE)\n\n    # train model\n    train(cnn, train_dataloader, loss_fn, optimiser, device, EPOCHS)\n\n    # save model\n    torch.save(cnn.state_dict(), \"feedforwardnet.pth\")\n    print(\"Trained feed forward net saved at feedforwardnet.pth\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lass_mapping = [\n    \"air_conditioner\",\n    \"car_horn\",\n    \"children_playing\",\n    \"dog_bark\",\n    \"drilling\",\n    \"engine_idling\",\n    \"gun_shot\",\n    \"jackhammer\",\n    \"siren\",\n    \"street_music\"\n]\n\n\ndef predict(model, input, target, class_mapping):\n    model.eval()\n    with torch.no_grad():\n        predictions = model(input)\n        # Tensor (1, 10) -> [ [0.1, 0.01, ..., 0.6] ]\n        predicted_index = predictions[0].argmax(0)\n        predicted = class_mapping[predicted_index]\n        expected = class_mapping[target]\n    return predicted, expected\n\n\nif __name__ == \"__main__\":\n    # load back the model\n    cnn = CNNNetwork()\n    state_dict = torch.load(\"cnnnet.pth\")\n    cnn.load_state_dict(state_dict)\n\n    # load urban sound dataset dataset\n    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n        sample_rate=SAMPLE_RATE,\n        n_fft=1024,\n        hop_length=512,\n        n_mels=64\n    )\n\n    usd = UrbanSoundDataset(ANNOTATIONS_FILE,\n                            AUDIO_DIR,\n                            mel_spectrogram,\n                            SAMPLE_RATE,\n                            NUM_SAMPLES,\n                            \"cpu\")\n\n\n    # get a sample from the urban sound dataset for inference\n    input, target = usd[0][0], usd[0][1] # [batch size, num_channels, fr, time]\n    input.unsqueeze_(0)\n\n    # make an inference\n    predicted, expected = predict(cnn, input, target,\n                                  class_mapping)\n    print(f\"Predicted: '{predicted}', expected: '{expected}'\")","metadata":{},"execution_count":null,"outputs":[]}]}