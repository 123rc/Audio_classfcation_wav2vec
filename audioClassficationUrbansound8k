{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":928025,"sourceType":"datasetVersion","datasetId":500970}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torch.utils.data import Dataset\nimport pandas as pd\nimport torchaudio\nimport os\nimport torch","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-01T14:10:09.094240Z","iopub.execute_input":"2024-07-01T14:10:09.094666Z","iopub.status.idle":"2024-07-01T14:10:13.631730Z","shell.execute_reply.started":"2024-07-01T14:10:09.094631Z","shell.execute_reply":"2024-07-01T14:10:13.630435Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class UrbanSoundDataset(Dataset):\n\n    def __init__(self,\n                 annotations_file,\n                 audio_dir,\n                 transformation,\n                 target_sample_rate,\n                 num_samples):\n        self.annotations = pd.read_csv(annotations_file)\n        self.audio_dir = audio_dir\n        self.transformation = transformation\n        self.target_sample_rate = target_sample_rate\n        self.num_samples = num_samples\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        audio_sample_path = self._get_audio_sample_path(index)\n        label = self._get_audio_sample_label(index)\n        signal, sr = torchaudio.load(audio_sample_path)\n        signal = self._resample_if_necessary(signal, sr)\n        signal = self._mix_down_if_necessary(signal)\n        signal = self._cut_if_necessary(signal)\n        signal = self._right_pad_if_necessary(signal)\n        signal = self.transformation(signal)\n        return signal, label\n\n    def _cut_if_necessary(self, signal):\n        if signal.shape[1] > self.num_samples:\n            signal = signal[:, :self.num_samples]\n        return signal\n\n    def _right_pad_if_necessary(self, signal):\n        length_signal = signal.shape[1]\n        if length_signal < self.num_samples:\n            num_missing_samples = self.num_samples - length_signal\n            last_dim_padding = (0, num_missing_samples)\n            signal = torch.nn.functional.pad(signal, last_dim_padding)\n        return signal\n\n    def _resample_if_necessary(self, signal, sr):\n        if sr != self.target_sample_rate:\n            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n            signal = resampler(signal)\n        return signal\n\n    def _mix_down_if_necessary(self, signal):\n        if signal.shape[0] > 1:\n            signal = torch.mean(signal, dim=0, keepdim=True)\n        return signal\n\n    def _get_audio_sample_path(self, index):\n        fold = f\"fold{self.annotations.iloc[index, 5]}\"\n        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[\n            index, 0])\n        return path\n\n    def _get_audio_sample_label(self, index):\n        return self.annotations.iloc[index, 6]","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:10:16.447163Z","iopub.execute_input":"2024-07-01T14:10:16.447807Z","iopub.status.idle":"2024-07-01T14:10:16.465764Z","shell.execute_reply.started":"2024-07-01T14:10:16.447772Z","shell.execute_reply":"2024-07-01T14:10:16.464581Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    ANNOTATIONS_FILE = \"/kaggle/input/urbansound8k/UrbanSound8K.csv\"\n    AUDIO_DIR = \"/kaggle/input/urbansound8k\"\n    SAMPLE_RATE = 22050\n    NUM_SAMPLES = 22050\n\n    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n        sample_rate=SAMPLE_RATE,\n        n_fft=1024,\n        hop_length=512,\n        n_mels=64\n    )\n\n    usd = UrbanSoundDataset(ANNOTATIONS_FILE,\n                            AUDIO_DIR,\n                            mel_spectrogram,\n                            SAMPLE_RATE,\n                            NUM_SAMPLES)\n    print(f\"There are {len(usd)} samples in the dataset.\")\n    signal, label = usd[1]","metadata":{"execution":{"iopub.status.busy":"2024-07-01T14:11:41.634912Z","iopub.execute_input":"2024-07-01T14:11:41.636432Z","iopub.status.idle":"2024-07-01T14:11:42.119111Z","shell.execute_reply.started":"2024-07-01T14:11:41.636376Z","shell.execute_reply":"2024-07-01T14:11:42.117936Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"There are 8732 samples in the dataset.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}